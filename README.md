# ğŸ” Semantic Retrieval System using FAISS

A complete, production-ready semantic retrieval system for RAG applications.

## ğŸ“‹ Overview

This system:
- **Ingests** raw textual knowledge from `.txt` files
- **Converts** text into dense vector embeddings
- **Stores** embeddings in a FAISS index
- **Retrieves** the most semantically similar text chunks for any query

> **Note**: This is a **RETRIEVER only** â€” RAG-ready, but without generation.

---

## ğŸ—ï¸ Architecture

```
Raw Text Files (.txt)
        â†“
Text Normalization (whitespace, newlines)
        â†“
Chunking + Metadata (~225 words, 50 overlap)
        â†“
Embedding Model (all-MiniLM-L6-v2)
        â†“
Vector Embeddings (384 dimensions, normalized)
        â†“
FAISS Index (IndexFlatIP for cosine similarity)
        â†“
Semantic Retrieval (Top-K similar chunks)
```

---

## ğŸ“ Project Structure

```
STIAI/
â”œâ”€â”€ .venv/                   # Virtual environment
â”œâ”€â”€ data/                    # Put your .txt files here
â”‚   â”œâ”€â”€ document1.txt
â”‚   â”œâ”€â”€ document2.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ index/                   # Generated artifacts
â”‚   â”œâ”€â”€ knowledge.faiss      # FAISS vector index
â”‚   â”œâ”€â”€ documents.pkl        # Chunk texts
â”‚   â””â”€â”€ meta.pkl             # Chunk metadata
â”œâ”€â”€ semantic_retriever.py    # Main Python script
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Step 1: Create & Activate Virtual Environment

```bash
# Create virtual environment
python -m venv .venv

# Activate (Windows)
.venv\Scripts\activate

# Activate (Linux/Mac)
source .venv/bin/activate
```

### Step 2: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 3: Run the System

```bash
python semantic_retriever.py
```

---

## ğŸ“¦ Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| `faiss-cpu` | â‰¥1.7.0 | Vector similarity search |
| `sentence-transformers` | â‰¥2.2.0 | Text embeddings |
| `numpy` | â‰¥1.21.0 | Numerical operations |

---

## ğŸ”§ Configuration

Edit the `RetrieverConfig` class to customize:

```python
@dataclass
class RetrieverConfig:
    data_folder: str = "data"           # Input folder
    output_folder: str = "index"        # Output folder
    chunk_size_words: int = 225         # Words per chunk
    chunk_overlap_words: int = 50       # Overlap words
    model_name: str = "all-MiniLM-L6-v2"  # Embedding model
```

---

## ğŸ“ Usage Examples

### Building the Index

```python
from semantic_retriever import build_index_pipeline, RetrieverConfig

config = RetrieverConfig(
    data_folder="data",
    output_folder="index"
)

build_index_pipeline(config)
```

### Retrieving Documents

```python
from semantic_retriever import SemanticRetriever, RetrieverConfig

# Initialize and load
retriever = SemanticRetriever(RetrieverConfig())
retriever.load()

# Query
query = "What is machine learning?"
results = retriever.retrieve(query, top_k=5)

# Display results
for result in results:
    print(f"Score: {result.similarity_score:.4f}")
    print(f"Source: {result.source_file}")
    print(f"Preview: {result.text_preview}")
    print()
```

---

## ğŸ“Š Output Format

Each retrieval result contains:

| Field | Type | Description |
|-------|------|-------------|
| `similarity_score` | float | Cosine similarity (0-1) |
| `source_file` | str | Original filename |
| `chunk_id` | int | Chunk index within file |
| `text_preview` | str | First 150 characters |
| `full_text` | str | Complete chunk text |

---

## ğŸ“º Expected Output

When you run `python semantic_retriever.py`, you'll see:

```
ğŸ“‚ STEP 1: DATA INGESTION
============================================================
  âœ“ Loaded: machine_learning.txt (1,639 characters)
  âœ“ Loaded: deep_learning.txt (1,607 characters)
  ...

ğŸ”§ STEP 2: TEXT PREPROCESSING
============================================================
  âœ“ machine_learning.txt: 1,639 â†’ 1,580 chars

âœ‚ï¸  STEP 3: TEXT CHUNKING
============================================================
  ğŸ“ Chunk size: ~225 words
  âœ“ machine_learning.txt: 2 chunks

ğŸ§  STEP 4: LOADING EMBEDDING MODEL
============================================================
  âœ“ Model loaded successfully!
  ğŸ“ Embedding dimension: 384

ğŸ”¢ GENERATING EMBEDDINGS
============================================================
  âœ“ Generated 6 embeddings

ğŸ—ï¸  STEP 5: BUILDING FAISS INDEX
============================================================
  âœ“ Index built successfully!

ğŸ’¾ STEP 6: SAVING ARTIFACTS
============================================================
  âœ“ Saved FAISS index â†’ index/knowledge.faiss

ğŸ” SEMANTIC RETRIEVAL
============================================================
  ğŸ“ Query: "What are the different types of machine learning?"
  âœ“ Found 3 relevant chunks

ğŸ“‹ RETRIEVAL RESULTS
============================================================
ğŸ† Result #1
ğŸ“„ Source: machine_learning.txt | Chunk #0
ğŸ“Š Similarity: 0.7823
```

---

## âš ï¸ Error Handling

| Error | Behavior |
|-------|----------|
| Empty data folder | Raises `ValueError` with clear message |
| Missing index files | Raises `FileNotFoundError` with instructions |
| Invalid FAISS scores | Skips with warning, continues retrieval |
| top_k > documents | Auto-adjusts to available documents |

---

## ğŸ”„ Adding Your Own Data

1. **Delete** sample files in `data/`
2. **Add** your own `.txt` files to `data/`
3. **Run** `python semantic_retriever.py` again

---

## ğŸ”„ Extending to RAG

Integrate with any LLM:

```python
def rag_answer(query, retriever, llm):
    # 1. Retrieve
    results = retriever.retrieve(query, top_k=5)
    
    # 2. Build context
    context = "\n\n".join([r.full_text for r in results])
    
    # 3. Generate
    prompt = f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:"
    answer = llm.generate(prompt)
    
    return answer, results
```

---

## ğŸ“ˆ Performance Tips

1. **Larger datasets**: Use `IndexIVFFlat` for faster approximate search
2. **GPU acceleration**: Install `faiss-gpu` instead of `faiss-cpu`
3. **Better embeddings**: Try larger models like `all-mpnet-base-v2`
4. **Chunk tuning**: Adjust chunk size based on your document structure

---

## ğŸ“„ License

MIT License - Feel free to use and modify for your projects.

---

**Built with â¤ï¸ for Information Retrieval and RAG systems**
